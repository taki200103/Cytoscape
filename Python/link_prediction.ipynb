{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HOANG\\AppData\\Local\\Temp\\ipykernel_9528\\2345822663.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_edges_tensor = torch.tensor(negative_edges.T, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1.0756607055664062, Accuracy: 0.6530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HOANG\\AppData\\Local\\Temp\\ipykernel_9528\\2345822663.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_edges_tensor = torch.tensor(negative_edges.T, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 0.9846515655517578, Accuracy: 0.6817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HOANG\\AppData\\Local\\Temp\\ipykernel_9528\\2345822663.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_edges_tensor = torch.tensor(negative_edges.T, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Loss: 0.9188287258148193, Accuracy: 0.7573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HOANG\\AppData\\Local\\Temp\\ipykernel_9528\\2345822663.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_edges_tensor = torch.tensor(negative_edges.T, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Loss: 0.8894703984260559, Accuracy: 0.7199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HOANG\\AppData\\Local\\Temp\\ipykernel_9528\\2345822663.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_edges_tensor = torch.tensor(negative_edges.T, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 0.8697546720504761, Accuracy: 0.7779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HOANG\\AppData\\Local\\Temp\\ipykernel_9528\\2345822663.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_edges_tensor = torch.tensor(negative_edges.T, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60, Loss: 0.8561029434204102, Accuracy: 0.7816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HOANG\\AppData\\Local\\Temp\\ipykernel_9528\\2345822663.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_edges_tensor = torch.tensor(negative_edges.T, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70, Loss: 0.8483686447143555, Accuracy: 0.7889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HOANG\\AppData\\Local\\Temp\\ipykernel_9528\\2345822663.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_edges_tensor = torch.tensor(negative_edges.T, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80, Loss: 0.8427889943122864, Accuracy: 0.8180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HOANG\\AppData\\Local\\Temp\\ipykernel_9528\\2345822663.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_edges_tensor = torch.tensor(negative_edges.T, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90, Loss: 0.8344252109527588, Accuracy: 0.8298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HOANG\\AppData\\Local\\Temp\\ipykernel_9528\\2345822663.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  negative_edges_tensor = torch.tensor(negative_edges.T, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.8278007507324219, Accuracy: 0.7962\n",
      "Final Accuracy: 0.8073\n",
      "M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'link_predict.pth'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# üìå B∆∞·ªõc 1: ƒê·ªçc d·ªØ li·ªáu t·ª´ file CSV\n",
    "nodes_df = pd.read_csv(r\"E:\\OOP\\Project_OOP\\Python\\arixv\\train_nodes.csv\")\n",
    "edges_df = pd.read_csv(r\"E:\\OOP\\Project_OOP\\Python\\arixv\\train_edges.csv\")\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√†nh tensor\n",
    "node_features = torch.tensor(nodes_df.iloc[:, 2:].values, dtype=torch.float)\n",
    "edge_index = torch.tensor(edges_df.values.T, dtype=torch.long)\n",
    "\n",
    "# T·∫°o ƒë·ªëi t∆∞·ª£ng Data cho PyTorch Geometric\n",
    "data = Data(x=node_features, edge_index=edge_index)\n",
    "\n",
    "# üìå B∆∞·ªõc 2: Chia d·ªØ li·ªáu th√†nh train/test edges\n",
    "edges = edges_df.values\n",
    "train_edges, test_edges = train_test_split(edges, test_size=0.2, random_state=42)\n",
    "\n",
    "# üìå B∆∞·ªõc 3: T·∫°o negative edges\n",
    "def create_negative_edges(num_nodes, existing_edges, num_samples):\n",
    "    existing_edges_set = set(map(tuple, existing_edges))\n",
    "    negative_edges = set()\n",
    "\n",
    "    while len(negative_edges) < num_samples:\n",
    "        u, v = random.randint(0, num_nodes - 1), random.randint(0, num_nodes - 1)\n",
    "        if u != v and (u, v) not in existing_edges_set and (v, u) not in existing_edges_set:\n",
    "            negative_edges.add((u, v))\n",
    "    \n",
    "    return np.array(list(negative_edges))\n",
    "\n",
    "num_nodes = node_features.shape[0]\n",
    "negative_test_edges = create_negative_edges(num_nodes, test_edges, len(test_edges))\n",
    "\n",
    "# üìå B∆∞·ªõc 4: ƒê·ªãnh nghƒ©a m√¥ h√¨nh GCN\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# üìå B∆∞·ªõc 5: ƒê·ªãnh nghƒ©a l·ªõp LinkPredictor\n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_dim * 2, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, z, edge_index):\n",
    "        z_u = z[edge_index[0]]\n",
    "        z_v = z[edge_index[1]]\n",
    "        edge_embeddings = torch.cat([z_u, z_v], dim=1)\n",
    "        return torch.sigmoid(self.mlp(edge_embeddings))\n",
    "\n",
    "# üìå B∆∞·ªõc 6: Kh·ªüi t·∫°o m√¥ h√¨nh\n",
    "in_channels = node_features.shape[1]\n",
    "hidden_channels = 64\n",
    "out_channels = 32\n",
    "\n",
    "gcn_model = GCN(in_channels, hidden_channels, out_channels)\n",
    "link_predictor = LinkPredictor(out_channels)\n",
    "\n",
    "# üìå B∆∞·ªõc 7: ƒê·ªãnh nghƒ©a h√†m hu·∫•n luy·ªán\n",
    "def train(model, predictor, data, train_edges, test_edges, negative_edges, optimizer, epochs=100):\n",
    "    model.train()\n",
    "    predictor.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Tr√≠ch xu·∫•t node embeddings\n",
    "        z = model(data.x, data.edge_index)\n",
    "        \n",
    "        # Chuy·ªÉn train_edges th√†nh tensor\n",
    "        positive_edges = torch.tensor(train_edges.T, dtype=torch.long)\n",
    "        negative_edges = create_negative_edges(data.x.shape[0], train_edges, len(train_edges))\n",
    "        negative_edges = torch.tensor(negative_edges.T, dtype=torch.long)\n",
    "        \n",
    "        # D·ª± ƒëo√°n li√™n k·∫øt\n",
    "        pos_preds = predictor(z, positive_edges)\n",
    "        neg_preds = predictor(z, negative_edges)\n",
    "        \n",
    "        # T√≠nh loss\n",
    "        loss = F.binary_cross_entropy(pos_preds, torch.ones_like(pos_preds)) + \\\n",
    "               F.binary_cross_entropy(neg_preds, torch.zeros_like(neg_preds))\n",
    "        \n",
    "        # Lan truy·ªÅn ng∆∞·ª£c\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # In loss v√† accuracy m·ªói 10 epoch\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            accuracy = evaluate(model, predictor, data, test_edges, negative_edges)\n",
    "            print(f'Epoch {epoch + 1}, Loss: {loss.item()}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# üìå B∆∞·ªõc 8: Kh·ªüi t·∫°o optimizer v√† hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "optimizer = torch.optim.Adam(list(gcn_model.parameters()) + list(link_predictor.parameters()), lr=0.01)\n",
    "train(gcn_model, link_predictor, data, train_edges, test_edges, negative_test_edges, optimizer)\n",
    "\n",
    "# üìå B∆∞·ªõc 9: ƒê√°nh gi√° m√¥ h√¨nh sau khi hu·∫•n luy·ªán\n",
    "accuracy = evaluate(gcn_model, link_predictor, data, test_edges, negative_test_edges)\n",
    "print(f\"Final Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# üìå B∆∞·ªõc 10: L∆∞u m√¥ h√¨nh\n",
    "torch.save({\n",
    "    'gcn_model': gcn_model.state_dict(),\n",
    "    'link_predictor': link_predictor.state_dict()\n",
    "}, 'link_predict.pth')\n",
    "\n",
    "print(\"M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'link_predict.pth'\")\n",
    "\n",
    "# üìå B∆∞·ªõc 11: ƒê·ªãnh nghƒ©a h√†m ƒë√°nh gi√° accuracy\n",
    "def evaluate(model, predictor, data, test_edges, negative_edges):\n",
    "    model.eval()\n",
    "    predictor.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Tr√≠ch xu·∫•t node embeddings\n",
    "        z = model(data.x, data.edge_index)\n",
    "        \n",
    "        # Chuy·ªÉn ƒë·ªïi edges th√†nh tensor\n",
    "        test_edges_tensor = torch.tensor(test_edges.T, dtype=torch.long)\n",
    "        negative_edges_tensor = torch.tensor(negative_edges.T, dtype=torch.long)\n",
    "        \n",
    "        # D·ª± ƒëo√°n li√™n k·∫øt\n",
    "        positive_preds = predictor(z, test_edges_tensor)\n",
    "        negative_preds = predictor(z, negative_edges_tensor)\n",
    "        \n",
    "        # G√°n nh√£n\n",
    "        all_preds = torch.cat([positive_preds, negative_preds])\n",
    "        all_labels = torch.cat([torch.ones_like(positive_preds), torch.zeros_like(negative_preds)])\n",
    "        \n",
    "        # T√≠nh accuracy\n",
    "        accuracy = accuracy_score(all_labels.cpu(), (all_preds.cpu() > 0.5).int())\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.09416823089122772\n",
      "Epoch 10, Loss: 0.01551192905753851\n",
      "Epoch 20, Loss: 0.012615996412932873\n",
      "Epoch 30, Loss: 0.011646936647593975\n",
      "Epoch 40, Loss: 0.010929622687399387\n",
      "Epoch 50, Loss: 0.010382021777331829\n",
      "Epoch 60, Loss: 0.009937699884176254\n",
      "Epoch 70, Loss: 0.009567917324602604\n",
      "Epoch 80, Loss: 0.009255164302885532\n",
      "Epoch 90, Loss: 0.008993063122034073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HOANG\\AppData\\Local\\Temp\\ipykernel_19832\\756670558.py:96: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# S·ª≠a l·∫°i class GCN\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, in_channels)  # out_channels = in_channels\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "def map_edge_index_to_new_nodes(edge_index, existing_node_count, new_node_count):\n",
    "    # L·ªçc b·ªè c√°c c·∫°nh c√≥ ch·ªâ s·ªë node v∆∞·ª£t qu√° s·ªë node hi·ªán c√≥\n",
    "    max_node_idx = existing_node_count + new_node_count - 1\n",
    "    valid_edges = (edge_index[0] <= max_node_idx) & (edge_index[1] <= max_node_idx)\n",
    "    edge_index = edge_index[:, valid_edges]\n",
    "    \n",
    "    # √Ånh x·∫° node\n",
    "    node_map = {i: i for i in range(existing_node_count)}\n",
    "    node_map.update({existing_node_count + i: existing_node_count + i for i in range(new_node_count)})\n",
    "    \n",
    "    edge_index_mapped = edge_index.clone()\n",
    "    edge_index_mapped[0] = torch.tensor([node_map.get(int(x), x) for x in edge_index[0]], dtype=torch.long)\n",
    "    edge_index_mapped[1] = torch.tensor([node_map.get(int(x), x) for x in edge_index[1]], dtype=torch.long)\n",
    "    \n",
    "    return edge_index_mapped\n",
    "\n",
    "# S·ª≠a l·∫°i h√†m load_and_train_model\n",
    "def load_and_train_model(train_edges_file, train_nodes_file, new_nodes_file=None):\n",
    "    # ƒê·ªçc v√† x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "    edges_df = pd.read_csv(train_edges_file)\n",
    "    nodes_df = pd.read_csv(train_nodes_file)\n",
    "    \n",
    "    # ƒê·∫£m b·∫£o c√°c ch·ªâ s·ªë trong edges_df kh√¥ng v∆∞·ª£t qu√° s·ªë node\n",
    "    max_node_id = len(nodes_df) - 1\n",
    "    edges_df = edges_df[\n",
    "        (edges_df['source'] <= max_node_id) & \n",
    "        (edges_df['target'] <= max_node_id)\n",
    "    ]\n",
    "\n",
    "    # ƒê·ªçc th√™m node m·ªõi n·∫øu c√≥\n",
    "    if new_nodes_file:\n",
    "        new_nodes_df = pd.read_csv(new_nodes_file)\n",
    "        new_node_features = torch.tensor(new_nodes_df.iloc[:, 1:].values, dtype=torch.float)\n",
    "    else:\n",
    "        new_node_features = torch.tensor([], dtype=torch.float)\n",
    "\n",
    "    # S·ªë l∆∞·ª£ng node trong ƒë·ªì th·ªã hi·ªán t·∫°i\n",
    "    existing_node_count = len(nodes_df)\n",
    "    all_node_features = torch.tensor(nodes_df.iloc[:, 1:].values, dtype=torch.float)\n",
    "\n",
    "    # √Ånh x·∫° l·∫°i ch·ªâ s·ªë c√°c c·∫°nh\n",
    "    edge_index = torch.tensor(edges_df.values.T, dtype=torch.long)\n",
    "    edge_index = map_edge_index_to_new_nodes(edge_index, existing_node_count, new_nodes_df.shape[0] if new_nodes_file else 0)\n",
    "\n",
    "    # Gh√©p c√°c node m·ªõi v√†o node features\n",
    "    all_node_features = torch.cat([all_node_features, new_node_features], dim=0)\n",
    "\n",
    "    # T·∫°o ƒë·ªëi t∆∞·ª£ng d·ªØ li·ªáu cho PyTorch Geometric\n",
    "    data = Data(x=all_node_features, edge_index=edge_index)\n",
    "\n",
    "    # Kh·ªüi t·∫°o model v·ªõi out_channels = in_channels\n",
    "    model = GCN(in_channels=all_node_features.shape[1], hidden_channels=64)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # Hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "    model.train()\n",
    "    for epoch in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = F.mse_loss(out, data.x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "    # L∆∞u m√¥ h√¨nh\n",
    "    torch.save(model.state_dict(), \"link_predict.pth\")\n",
    "\n",
    "    return model, data\n",
    "\n",
    "# S·ª≠a l·∫°i h√†m predict_with_trained_model \n",
    "def predict_with_trained_model(model_path, data, new_nodes_file):\n",
    "    # T·∫£i m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán\n",
    "    model = GCN(in_channels=data.x.shape[1], hidden_channels=64)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    # ƒê·ªçc node m·ªõi v√† ƒë·∫£m b·∫£o s·ªë features gi·ªëng nhau\n",
    "    new_nodes_df = pd.read_csv(new_nodes_file)\n",
    "    expected_features = data.x.shape[1]  # S·ªë features c·ªßa nodes hi·ªán t·∫°i\n",
    "    \n",
    "    # Ch·ªâ l·∫•y s·ªë c·ªôt features gi·ªëng v·ªõi nodes hi·ªán t·∫°i\n",
    "    new_node_features = torch.tensor(new_nodes_df.iloc[:, 1:expected_features+1].values, dtype=torch.float)\n",
    "\n",
    "    # Gh√©p c√°c node m·ªõi v√†o node_features hi·ªán t·∫°i\n",
    "    all_node_features = torch.cat([data.x, new_node_features], dim=0)\n",
    "\n",
    "    # D·ª± ƒëo√°n embedding cho t·∫•t c·∫£ c√°c node, bao g·ªìm node m·ªõi\n",
    "    edge_index = data.edge_index\n",
    "    with torch.no_grad():\n",
    "        output_embeddings = model(all_node_features, edge_index)\n",
    "\n",
    "    return output_embeddings, all_node_features\n",
    "\n",
    "def compute_similarity_and_create_edges(output_embeddings, existing_node_count, edges_df, threshold=0.5, max_edges=2):\n",
    "    embeddings_np = output_embeddings if isinstance(output_embeddings, np.ndarray) else output_embeddings.numpy()\n",
    "    \n",
    "    new_nodes_embeddings = embeddings_np[existing_node_count:]\n",
    "    existing_nodes_embeddings = embeddings_np[:existing_node_count]\n",
    "    \n",
    "    similarities = cosine_similarity(new_nodes_embeddings, existing_nodes_embeddings)\n",
    "    \n",
    "    new_edges = []\n",
    "    for i in range(len(new_nodes_embeddings)):\n",
    "        new_node_idx = i + existing_node_count\n",
    "        # Get top 2 most similar existing nodes\n",
    "        top_similar_indices = np.argsort(similarities[i])[-max_edges:]\n",
    "        \n",
    "        for target_idx in top_similar_indices:\n",
    "            if similarities[i][target_idx] > threshold:\n",
    "                new_edges.append([new_node_idx, int(target_idx)])\n",
    "    \n",
    "    # Combine old and new edges\n",
    "    old_edges = edges_df.values.tolist()\n",
    "    all_edges = old_edges + new_edges\n",
    "    \n",
    "    return all_edges\n",
    "\n",
    "# Main Code\n",
    "if __name__ == \"__main__\":\n",
    "    # ƒê·ªçc d·ªØ li·ªáu ban ƒë·∫ßu v√† hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "    model, data = load_and_train_model(\"E:/OOP/Project_OOP/Python/link_process/edges_user.csv\", \n",
    "                                       \"E:/OOP/Project_OOP/Python/link_process/nodes_user.csv\")\n",
    "    \n",
    "    # D·ª± ƒëo√°n v·ªõi node m·ªõi\n",
    "    output_embeddings, all_node_features = predict_with_trained_model(\"link_predict.pth\", data, \"E:/OOP/Project_OOP/Python/link_process/node_difference.csv\")\n",
    "    \n",
    "    # L∆∞u embeddings v√† c√°c node m·ªõi v√†o CSV\n",
    "    output_node_ids = torch.arange(all_node_features.shape[0]).numpy()\n",
    "    output_embeddings = output_embeddings.numpy()\n",
    "\n",
    "    # L∆∞u node embeddings v√†o CSV\n",
    "    node_embeddings_df = pd.DataFrame(output_embeddings)\n",
    "    node_embeddings_df.insert(0, 'node_id', output_node_ids)\n",
    "    node_embeddings_df.to_csv(r'E:\\OOP\\Project_OOP\\Python\\link_process\\all_embeddings.csv', index=False)\n",
    "\n",
    "    # Read original edges\n",
    "    edges_df = pd.read_csv(\"E:/OOP/Project_OOP/Python/link_process/edges_user.csv\")\n",
    "    \n",
    "    # Get all edges including predictions\n",
    "    all_edges = compute_similarity_and_create_edges(\n",
    "        output_embeddings,\n",
    "        len(data.x),\n",
    "        edges_df,\n",
    "        threshold=0.5\n",
    "    )\n",
    "    \n",
    "    # Save all edges\n",
    "    all_edges_df = pd.DataFrame(all_edges, columns=['source', 'target'])\n",
    "    all_edges_df.to_csv(r'E:\\OOP\\Project_OOP\\Python\\link_process\\all_edges.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are connected to Cytoscape!\n",
      "   source  target\n",
      "0     339     175\n",
      "1     368     460\n",
      "2     340     380\n",
      "3      60      46\n",
      "4     442      22\n",
      "Applying default style...\n",
      "Applying preferred layout\n",
      "Network created successfully!\n"
     ]
    }
   ],
   "source": [
    "import py4cytoscape as p4c\n",
    "import pandas as pd\n",
    "\n",
    "# K·∫øt n·ªëi v·ªõi Cytoscape\n",
    "p4c.cytoscape_ping()\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ t·ªáp CSV (source, target)\n",
    "edges_df = pd.read_csv(r'E:\\OOP\\Project_OOP\\Python\\link_process\\all_edges.csv')\n",
    "\n",
    "# Ki·ªÉm tra d·ªØ li·ªáu ƒë√£ ƒë·ªçc\n",
    "print(edges_df.head())\n",
    "\n",
    "# L·∫•y danh s√°ch c√°c n√∫t t·ª´ c√°c c·ªôt 'source' v√† 'target'\n",
    "nodes_list = pd.concat([edges_df['source'], edges_df['target']]).unique()\n",
    "\n",
    "# T·∫°o DataFrame cho c√°c n√∫t\n",
    "nodes_df = pd.DataFrame(nodes_list, columns=['id'])\n",
    "\n",
    "# Th√™m c·ªôt 'interaction' n·∫øu ch∆∞a c√≥\n",
    "if 'interaction' not in edges_df.columns:\n",
    "    edges_df['interaction'] = 'interacts'\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi c√°c gi√° tr·ªã th√†nh chu·ªói\n",
    "edges_df['source'] = edges_df['source'].astype(str)\n",
    "edges_df['target'] = edges_df['target'].astype(str)\n",
    "nodes_df['id'] = nodes_df['id'].astype(str)\n",
    "\n",
    "# T·∫°o m·∫°ng t·ª´ c√°c DataFrame c·ªßa n√∫t v√† c·∫°nh\n",
    "network = p4c.create_network_from_data_frames(nodes_df, edges_df, title='Link Network', collection='My Collection')\n",
    "\n",
    "\n",
    "# Ki·ªÉm tra m·∫°ng ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng\n",
    "print(\"Network created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      SUID shared name   id         0         1         2         3         4  \\\n",
      "7169  7169          98   98 -0.109298  0.061719 -0.220922  0.081481  0.026675   \n",
      "6146  6146          26   26 -0.110200 -0.059621 -0.211268  0.110036  0.061998   \n",
      "7172  7172         335  335 -0.149694  0.062972 -0.180110  0.112999  0.134093   \n",
      "6149  6149          73   73 -0.136004 -0.008596 -0.218698  0.026196  0.039039   \n",
      "7175  7175         112  112 -0.087737  0.026812 -0.100444 -0.105991  0.040443   \n",
      "...    ...         ...  ...       ...       ...       ...       ...       ...   \n",
      "6137  6137          11   11 -0.160092 -0.016777 -0.163568 -0.073726  0.068310   \n",
      "7163  7163         265  265 -0.139076  0.043572 -0.180990 -0.108719  0.035920   \n",
      "6140  6140          37   37 -0.180965 -0.072114 -0.221447 -0.091723  0.068965   \n",
      "7166  7166         171  171 -0.184844  0.070447 -0.207689  0.069398 -0.003302   \n",
      "6143  6143         345  345 -0.087531 -0.034971 -0.164422 -0.094609  0.034036   \n",
      "\n",
      "             5         6  ...       120       121       122       123  \\\n",
      "7169 -0.044180 -0.440302  ... -0.168889 -0.087217  0.359433  0.062709   \n",
      "6146 -0.080716 -0.443598  ... -0.216672 -0.156502  0.196647  0.151186   \n",
      "7172 -0.071373 -0.502816  ... -0.221549 -0.150748  0.284995  0.086182   \n",
      "6149 -0.101940 -0.346470  ... -0.185280 -0.123503  0.251796 -0.009039   \n",
      "7175 -0.117876 -0.424842  ... -0.237424 -0.147770  0.295724 -0.148468   \n",
      "...        ...       ...  ...       ...       ...       ...       ...   \n",
      "6137 -0.026464 -0.351495  ... -0.198079 -0.146532  0.297342 -0.023993   \n",
      "7163 -0.052327 -0.383334  ... -0.179106 -0.097500  0.373278 -0.060809   \n",
      "6140 -0.052991 -0.375411  ... -0.200063 -0.140646  0.363869 -0.088958   \n",
      "7166 -0.005954 -0.527609  ... -0.193578 -0.149818  0.401726  0.083392   \n",
      "6143 -0.098295 -0.363515  ... -0.153460 -0.100876  0.338048 -0.128777   \n",
      "\n",
      "           124       125       126       127  name  selected  \n",
      "7169  0.230035  0.123697 -0.110688 -0.085560    98     False  \n",
      "6146  0.098060 -0.079280 -0.045969 -0.161978    26     False  \n",
      "7172  0.139276  0.038348 -0.028128 -0.181347   335     False  \n",
      "6149  0.103370  0.108190 -0.032604 -0.119261    73     False  \n",
      "7175  0.072677  0.031885  0.029294 -0.146996   112     False  \n",
      "...        ...       ...       ...       ...   ...       ...  \n",
      "6137  0.172100  0.054360  0.017269 -0.128470    11     False  \n",
      "7163  0.272782  0.056037  0.037791 -0.112779   265     False  \n",
      "6140  0.142790  0.073417 -0.027098 -0.116406    37     False  \n",
      "7166  0.302370  0.004388 -0.097762 -0.125043   171     False  \n",
      "6143  0.083560  0.147594 -0.091293 -0.077321   345     False  \n",
      "\n",
      "[412 rows x 133 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n",
      "c:\\Users\\HOANG\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4cytoscape\\tables.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = cvv\n"
     ]
    }
   ],
   "source": [
    "import py4cytoscape as cy\n",
    "import os\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n t·ªõi t·ªáp CSV\n",
    "file_path = r'E:\\OOP\\Project_OOP\\Python\\link_process\\all_embeddings.csv'\n",
    "\n",
    "# Ki·ªÉm tra xem t·ªáp c√≥ t·ªìn t·∫°i kh√¥ng\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"File not found: {file_path}\")\n",
    "else:\n",
    "    # T·∫£i d·ªØ li·ªáu b·∫£ng v√†o Cytoscape\n",
    "    try:\n",
    "        cy.load_table_data_from_file(file_path)\n",
    "        # Ki·ªÉm tra c√°c c·ªôt trong b·∫£ng d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c t·∫£i v√†o Cytoscape\n",
    "        print(cy.get_table_columns())\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading table data: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
